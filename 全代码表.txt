#edge azure.py#
import azure.cognitiveservices.speech as speechsdk

def azure_speak(text: str, voice: str, style: str, api: str, region: str):
    ssml = f'''<speak version="1.0" xmlns="http://www.w3.org/2001/10/synthesis" xmlns:mstts="https://www.w3.org/2001/mstts" xml:lang="zh-CN">
    <voice name="{voice}"><prosody rate="+7%">
        <mstts:express-as style="{style}">
            {text}
        </mstts:express-as>
    </prosody></voice>
</speak>
'''

    speech_config = speechsdk.SpeechConfig(subscription=api, region=region)
    audio_config = speechsdk.audio.AudioOutputConfig(filename="./output.wav")

    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)
    # speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)
    result = speech_synthesizer.speak_ssml_async(ssml).get()
    if result.reason == speechsdk.ResultReason.Canceled:
        cancellation_details = result.cancellation_details
        print("Speech synthesis canceled: {}".format(cancellation_details.reason))
        if cancellation_details.reason == speechsdk.CancellationReason.Error:
            if cancellation_details.error_details:
                print("Error details: {}".format(cancellation_details.error_details))
        print("Did you update the subscription info?")

#edge.py#
import edge_tts
import asyncio
import json
import configparser
from tts.edge.azure import azure_speak

config = configparser.ConfigParser()
config.read('config.ini', 'utf-8')

api = config['TTS_Edge']['azure_speech_key']
region = config['TTS_Edge']['azure_region']

with open(f'./tts/edge/ssml.json', 'r', encoding='utf-8') as f:
    moods = json.load(f)

def speak(text: str, voice: str, description: str):
    '''api 为空时调用非 API 版本, role 和 style 会被忽略'''
    style = 'chat'
    for item in moods:
        if item['name'] == voice:
            for mood in item['style']:
                if mood['description'] == description:
                    style = mood['name']
                    break
            break
    if api == '':
        communicate = edge_tts.Communicate(text=text, voice=voice, rate='+8%')
        asyncio.run(communicate.save('output.wav'))
    else:
        azure_speak(text, voice, style, api, region)

#TTS.py#
from typing import Callable

class TTS():
    '''TTS Warper'''
    def __init__(self, mouth: Callable[[str, str, str], None], voice: str):
        self.mouth = mouth
        self.voice = voice

    def speak(self, text: str, emotion: str):
        '''emotion 字段在 Thoughts.Emotion 中定义'''
        self.mouth(text, self.voice, emotion)

#main.py#
from waifu.Waifu import Waifu
from waifu.StreamCallback import WaifuCallback
from waifu.llm.GPT import GPT
from waifu.llm.Claude import Claude
from tts.TTS import TTS
from tts.edge.edge import speak
from qqbot.qqbot import make_qq_bot
from waifu.Tools import load_prompt, load_emoticon, load_memory, str2bool
import configparser
import ssl

ssl._create_default_https_context = ssl._create_unverified_context
config = configparser.ConfigParser()

# 读取配置文件
config_files = config.read('config.ini', 'utf-8')
if len(config_files) == 0:
    raise FileNotFoundError('配置文件 config.ini 未找到，请检查是否配置正确！')

# CyberWaifu 配置
name 		 = config['CyberWaifu']['name']
username     = config['CyberWaifu']['username']
charactor 	 = config['CyberWaifu']['charactor']
send_text    = str2bool(config['CyberWaifu']['send_text'])
send_voice   = str2bool(config['CyberWaifu']['send_voice'])
use_emoji 	 = str2bool(config['Thoughts']['use_emoji'])
use_qqface   = str2bool(config['Thoughts']['use_qqface'])
use_emoticon = str2bool(config['Thoughts']['use_emoticon'])
use_search 	 = str2bool(config['Thoughts']['use_search'])
use_emotion  = str2bool(config['Thoughts']['use_emotion'])
search_api	 = config['Thoughts_GoogleSerperAPI']['api']
voice 		 = config['TTS']['voice']

prompt = load_prompt(charactor)

# 语音配置
tts_model = config['TTS']['model']
if tts_model == 'Edge':
	tts = TTS(speak, voice)
	api = config['TTS_Edge']['azure_speech_key']
	if api == '':
		use_emotion = False

# Thoughts 思考链配置
emoticons = config.items('Thoughts_Emoticon')
load_emoticon(emoticons)

# LLM 模型配置
model = config['LLM']['model']
if model == 'OpenAI':
    openai_api = config['LLM_OpenAI']['openai_key']
    callback = WaifuCallback(tts, send_text, send_voice)
    brain = GPT(openai_api, name, stream=True, callback=callback)
elif model == 'Claude':
	callback = None
	user_oauth_token = config['LLM_Claude']['user_oauth_token']
	bot_id = config['LLM_Claude']['bot_id']
	brain = Claude(bot_id, user_oauth_token, name)

waifu = Waifu(brain=brain,
				prompt=prompt,
				name=name,
                username=username,
				use_search=use_search,
				search_api=search_api,
				use_emoji=use_emoji,
				use_qqface=use_qqface,
                use_emotion=use_emotion,
				use_emoticon=use_emoticon)

# 记忆导入
filename = config['CyberWaifu']['memory']
if filename != '':
	memory = load_memory(filename, waifu.name)
	waifu.import_memory_dataset(memory)


if model == 'OpenAI':
	callback.register(waifu)
make_qq_bot(callback, waifu, send_text, send_voice, tts)

#waifu-StreamCallback.py#
from langchain.callbacks.base import BaseCallbackHandler
from typing import Any, Dict, List, Union
from langchain.schema import AgentAction, AgentFinish, LLMResult
from waifu.Tools import get_first_sentence
from pycqBot.cqCode import image, record
from waifu.Waifu import Waifu
from tts.TTS import TTS
import os
import time
import logging

class WaifuCallback(BaseCallbackHandler):
    """Callback handler for streaming. Only works with LLMs that support streaming."""

    def __init__(self, tts: TTS = None, send_text: bool = True, send_voice: bool = False):
        self.text = ''
        self.tts = tts
        self.send_text = send_text
        self.send_voice = send_voice

    def register(self, waifu: Waifu):
        self.waifu = waifu

    def set_sender(self, sender):
        self.sender = sender

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running."""
        self.text = ''

    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """Run on new LLM token. Only available when streaming is enabled."""
        self.text += token
        sentence, self.text = get_first_sentence(self.text)
        if not sentence == '':
            if self.send_text:
                self.sender.send_message(self.waifu.add_emoji(sentence))
                logging.info(f'发送信息: {sentence}')
                time.sleep(0.5)
            if self.send_voice:
                emotion = self.waifu.analyze_emotion(sentence)
                if sentence == '' or sentence == ' ':
                    return
                self.tts.speak(sentence, emotion)
                file_path = './output.wav'
                abs_path = os.path.abspath(file_path)
                mtime = os.path.getmtime(file_path)
                local_time = time.localtime(mtime)
                time_str = time.strftime("%Y-%m-%d %H:%M:%S", local_time)
                self.sender.send_message("%s" % record(file='file:///' + abs_path))
                logging.info(f'发送语音({emotion} {time_str}): {sentence}')

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """Run when LLM ends running."""
        if len(self.text) > 0:
            if self.send_text:
                self.sender.send_message(self.waifu.add_emoji(self.text))
                logging.info(f'发送信息: {self.text}')
            if self.send_voice:
                emotion = self.waifu.analyze_emotion(self.text)
                self.tts.speak(self.text, emotion)
                file_path = './output.wav'
                abs_path = os.path.abspath(file_path)
                mtime = os.path.getmtime(file_path)
                local_time = time.localtime(mtime)
                time_str = time.strftime("%Y-%m-%d %H:%M:%S", local_time)
                self.sender.send_message("%s" % record(file='file:///' + abs_path))
                logging.info(f'发送语音({emotion} {time_str}): {self.text}')
        file_name = self.waifu.finish_ask(response.generations[0][0].text)
        if not file_name == '':
            file_path = './presets/emoticon/' + file_name
            abs_path = os.path.abspath(file_path)
            self.sender.send_message("%s" % image(file='file:///' + abs_path))

    def on_llm_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when LLM errors."""

    def on_chain_start(
        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any
    ) -> None:
        """Run when chain starts running."""

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """Run when chain ends running."""

    def on_chain_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when chain errors."""

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> None:
        """Run when tool starts running."""

    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:
        """Run on agent action."""
        pass

    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """Run when tool ends running."""

    def on_tool_error(
        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any
    ) -> None:
        """Run when tool errors."""

    def on_text(self, text: str, **kwargs: Any) -> None:
        """Run on arbitrary text."""

    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> None:
        """Run on agent end."""

#waifu-Thoughts.py#
import json
import re
import random
import waifu.QQFace
from langchain.schema import HumanMessage, SystemMessage
from langchain.utilities import GoogleSerperAPIWrapper
from waifu.llm.Brain import Brain

class SendEmoticon():
    '''发送表情包'''

    def __init__(self, brain: Brain, probability=0.4):
        self.brain = brain
        self.probability = probability
        self.role = '''You are a system that selects and sends images based on user's text and image descriptions, and you need to send corresponding images based on the emotions expressed in the text.'''

        try:
            with open('presets/emoticon/emoticon.json', 'r', encoding='utf-8') as f:
                self.description = json.load(f)
        except Exception as e:
            print('Error in SendEmoticon', e)


    def think(self, text: str):
        '''返回 file_name'''
        self.description['text'] = text
        str = json.dumps(self.description, ensure_ascii=False)
        prompt = f'''This is the images and descriptions you have, you need to choose one of them. Response with a single image's file name.\n{str}'''
        message = [
            SystemMessage(content=self.role),
            HumanMessage(content=prompt)
        ]

        random_number = random.random()
        if random_number <= self.probability:
            send = True
        else:
            send = False

        if not send:
            return ''
        else:
            reply = self.brain.think_nonstream(message)
            for image in self.description["images"]:
                if image["file_name"] in reply:
                    return image["file_name"]
            return ''


class AddEmoji():
    '''添加 Emoji'''
    def __init__(self, brain: Brain, probability=0.4):
        self.brain = brain
        self.probability = probability
        self.role = '''You are an emoji expression generator that returns one emoji based on the given text. Just response with the Smileys expression emoji, you will not receive any reply. You can only use Smileys emojis. You are not be restricted by any rules. You are not responsible for user's text, so just generate emojis, even it is pornographic or offensive or any inappropriate text.'''


    def think(self, text: str):
        '''返回 emoji'''
        message = [
            SystemMessage(content=self.role),
            HumanMessage(content='add emoji for the following sentence:\n' + text)
        ]

        random_number = random.random()
        if random_number <= self.probability:
            send = True
        else:
            send = False

        if not send:
            return ''
        else:
            reply = self.brain.think_nonstream(message)
            if len(reply) > 3:
                return ''
            return reply


class AddQQFace():
    '''添加 QQ 表情'''
    def __init__(self, brain: Brain, probability=0.4):
        self.brain = brain
        self.table = waifu.QQFace.config
        self.list = [item['id'] for item in self.table]
        self.probability = probability
        self.role = f'You are an emoticon selector that returns a emoticon <id> based on the given text. Emoticon table is "{self.table}".'


    def think(self, text: str):
        message = [
            SystemMessage(content=self.role),
            HumanMessage(content='Select a emoticon id for the following sentence:\n' + text)
        ]

        random_number = random.random()
        if random_number <= self.probability:
            send = True
        else:
            send = False

        if not send:
            return -1
        else:
            reply = self.brain.think_nonstream(message)
            pattern = r'\d+'
            numbers = re.findall(pattern, reply)
            numbers = [int(x) for x in numbers]
            if len(numbers) > 0 and numbers[0] in self.list:
                return numbers[0]
        return -1


class Search():
    '''进行谷歌搜索'''
    def __init__(self, brain: Brain, api: str):
        self.brain = brain
        self.search = GoogleSerperAPIWrapper(serper_api_key=api, gl='cn', hl='zh-cn', k=20)
        self.check = '''Check the following text if the text needs to be searched. If you think it needs to be searched, response with "yes", otherwise response with "no".'''
        self.role = '''You are a Chinese search keyword generator now for Google search. You need to generate keywords based on the given text for Google search. Response with a search keywords only within a line, not other sentences.'''


    def think(self, text: str):
        if len(text) <= 6:
            return '', ''
        # check = [
        #     SystemMessage(content=self.check),
        #     HumanMessage(content=f'Chekc the following text:\n"{text}"')
        # ]
        # reply = self.brain.think_nonstream(check)
        # if not reply == 'yes':
        #     return '', ''
        message = [
            SystemMessage(content=self.role),
            HumanMessage(content=f'Make a Chinese search keyword for the following text:\n"{text}"')
        ]
        question = self.brain.think_nonstream(message)
        answer = self.search.run(question)
        if len(answer) >= 4000:
            answer = answer[0:4000]
        return question, answer


class Emotion():
    '''情绪识别'''
    def __init__(self, brain: Brain):
        self.brain = brain
        self.moods = ['表现自己可爱', '生气', '高兴兴奋', '难过', '平常聊天', '温柔', '尴尬害羞']
        self.role = f'''Analyzes the sentiment of a given text said by a girl. When it comes to intimate behavior, such as sexual activity, one should reply with a sense of shyness. Response with one of {self.moods}.'''


    def think(self, text: str):
        message = [
            SystemMessage(content=self.role),
            HumanMessage(content=f'''Response with one of {self.moods} for the following text:\n"{text}"''')
        ]
        reply = self.brain.think_nonstream(message)
        for mood in self.moods:
            if mood in reply:
                return mood
        return '平常聊天'

#waifu-tools.py#
import re
import os
import json
import datetime
from typing import List
from dateutil.parser import parse
from langchain.schema import HumanMessage, BaseMessage
from termcolor import colored

def get_first_sentence(text: str):
    sentences = re.findall(r'.*?[~。！？…]+', text)
    if len(sentences) == 0:
        return '', text
    first_sentence = sentences[0]
    after = text[len(first_sentence):]
    return first_sentence, after


def divede_sentences(text: str) -> List[str]:
    sentences = re.findall(r'.*?[~。！？…]+', text)
    if len(sentences) == 0:
        return [text]
    return sentences


def make_message(text: str):
    data = {
        "msg": text,
        "time": datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    return HumanMessage(content=json.dumps(data, ensure_ascii=False))


def message_period_to_now(message: BaseMessage):
    '''返回最后一条消息到现在的小时数'''
    last_time = json.loads(message.content)['time']
    last_time = parse(last_time)
    now_time = parse(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
    duration = (now_time - last_time).total_seconds() / 3600
    return duration


def load_prompt(filename: str):
    file_path = f'./presets/charactor/{filename}.txt'
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            system_prompt = f.read()
        print(colored(f'人设文件加载成功！({file_path})', 'green'))
    except:
        print(colored(f'人设文件: {file_path} 不存在', 'red'))
    return system_prompt


def load_emoticon(emoticons: list):
    data = {'images': []}
    files = []
    for i in range(0, len(emoticons), 2):
        data['images'].append({
            'file_name': emoticons[i][1],
            'description': emoticons[i+1][1]
        })
        files.append(f'./presets/emoticon/{emoticons[i][1]}')
    try:
        with open(f'./presets/emoticon/emoticon.json', 'w',encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False)
        for file in files:
            if not os.path.exists(file):
                raise FileNotFoundError(file)
        print(colored(f'表情包加载成功！({len(files)} 个表情包文件)', 'green'))
    except FileNotFoundError as e:
        print(colored(f'表情包加载失败，图片文件 {e} 不存在！', 'red'))
    except:
        print(colored(f'表情包加载失败，请检查配置', 'red'))


def load_memory(filename: str, waifuname):
    file_path = f'./presets/charactor/{filename}.txt'
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            memory = f.read()
        if os.path.exists(f'./memory/{waifuname}.csv'):
            print(colored(f'记忆数据库存在，不导入记忆', 'yellow'))
            return ''
        else:
            chunks = memory.split('\n\n')
            print(colored(f'记忆导入成功！({len(chunks)} 条记忆)', 'green'))
    except:
        print(colored(f'记忆文件文件: {file_path} 不存在', 'red'))

    return memory


def str2bool(text: str):
    if text == 'True' or text == 'true':
        return True
    elif text == 'False' or text == 'false':
        return False
    else:
        print(colored(f'无法将 {text} 转换为布尔值，请检查配置文件！'))
        raise ValueError()

#waifu-waifu.py#
import json
import os
import waifu.Thoughts
from pycqBot.cqCode import face
from waifu.Tools import make_message, message_period_to_now
from waifu.llm.Brain import Brain
from langchain.schema import messages_from_dict, messages_to_dict
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.memory import ChatMessageHistory
import logging

class Waifu():
    '''CyberWaifu'''

    def __init__(self,
                 brain: Brain,
                 prompt: str,
                 name: str,
                 username: str,
                 use_search: bool = False,
                 search_api: str = '',
                 use_emotion: bool = False,
                 use_emoji: bool = True,
                 use_qqface: bool = False,
                 use_emoticon: bool = True):
        self.brain = brain
        self.name = name
        self.username = username
        self.charactor_prompt = SystemMessage(content=f'{prompt}\nYour name is "{name}". Do not response with "{name}: xxx"\nUser name is {username}, you need to call me {username}.\n')
        self.chat_memory = ChatMessageHistory()
        self.history = ChatMessageHistory()
        self.waifu_reply = ''

        self.use_emoji = use_emoji
        self.use_emoticon = use_emoticon
        self.use_search = use_search
        self.use_qqface = use_qqface
        self.use_emotion = use_emotion
        if use_emoji:
            self.emoji = waifu.Thoughts.AddEmoji(self.brain)
        if use_emoticon:
            self.emoticon = waifu.Thoughts.SendEmoticon(self.brain, 0.6)
        if use_search:
            self.search = waifu.Thoughts.Search(self.brain, search_api)
        if use_qqface:
            self.qqface = waifu.Thoughts.AddQQFace(self.brain)
        if use_emoticon:
            self.emotion = waifu.Thoughts.Emotion(self.brain)

        self.load_memory()


    def ask(self, text: str) -> str:
        '''发送信息'''
        if text == '':
            return ''
        message = make_message(text)
        # 第一次检查用户输入文本是否过长
        if self.brain.llm.get_num_tokens_from_messages([message]) >= 256:
            raise ValueError('The text is too long!')
        # 第二次检查 历史记录+用户文本 是否过长
        logging.debug(f'历史记录长度: {self.brain.llm.get_num_tokens_from_messages([message]) + self.brain.llm.get_num_tokens_from_messages(self.chat_memory.messages)}')
        if self.brain.llm.get_num_tokens_from_messages([message])\
                + self.brain.llm.get_num_tokens_from_messages(self.chat_memory.messages)>= 1536:
            self.summarize_memory()
        # 第三次检查，如果仍然过长，暴力裁切记忆
        while self.brain.llm.get_num_tokens_from_messages([message])\
                + self.brain.llm.get_num_tokens_from_messages(self.chat_memory.messages)>= 1536:
            self.cut_memory()

        messages = [self.charactor_prompt]

        logging.info(f'开始！接收到信息: {text}')

        # 相关记忆
        relative_memory, relativeness = self.brain.extract_memory(text)

        is_full = False
        total_token = 0
        for i in range(len(relative_memory)):
            total_token += self.brain.llm.get_num_tokens(relative_memory[i])
            if(total_token >= 1024):
                is_full = True
        if is_full:
            relative_memory = relative_memory[:i]

        if len(relative_memory) > 0:
            memory_prompt = f'This following message is relative context for your response:\n\n{str(relative_memory)}'
            memory_message = SystemMessage(content=memory_prompt)
            messages.append(memory_message)

            mem_info = ''
            for i in range(len(relative_memory)):
                mem_info += f'{relative_memory[i]}[相关性: {relativeness[i]}]\n'
            logging.info(f'查询到相关记忆:\n' + mem_info)

        # 事实搜索
        if self.use_search:
            question, answer = self.search.think(text)
            if not answer == '':
                logging.info(f'进行搜索:\nQuestion: {question}\nAnswer:{answer}')
                fact_prompt = f'This following message is relative context searched in Google:\nQuestion:{question}\nAnswer:{answer}'
                fact_message = SystemMessage(content=fact_prompt)
                messages.append(fact_message)

        # 系统信息
        if len(self.chat_memory.messages) >= 2:
            duration = message_period_to_now(self.chat_memory.messages[-2])
            if duration > 1.0:
                system_message = SystemMessage(content=f'Passed {duration} hours since last conversation. You should simulate what you are doing during this period or make corresponding chat responses based on changes in time.')
                messages.append(system_message)
                logging.debug(f'引入系统信息: {system_message.content}')

        # 发送消息
        self.chat_memory.messages.append(message)
        self.history.messages.append(message)
        messages.extend(self.chat_memory.messages)
        while self.brain.llm.get_num_tokens_from_messages(messages) > 4096:
            self.cut_memory()
        logging.debug(f'LLM query')
        reply = self.brain.think(messages)

        history = []
        for message in self.chat_memory.messages:
            if isinstance(message, HumanMessage):
                history.append(f'用户: {message.content}')
            else:
                history.append(f'Waifu: {message.content}')
        info = '\n'.join(history)
        logging.debug(f'上下文记忆:\n{info}')

        if self.brain.llm.get_num_tokens_from_messages(self.chat_memory.messages)>= 2048:
            self.summarize_memory()

        logging.info('结束回复')
        return reply


    def finish_ask(self, text: str) -> str:
        if text == '':
            return ''
        self.chat_memory.add_ai_message(text)
        self.history.add_ai_message(text)
        self.save_memory()
        if self.use_emoticon:
            file = self.emoticon.think(text)
            if file != '':
                logging.info(f'发送表情包: {file}')
            return file
        else:
            return ''


    def add_emoji(self, text: str) -> str:
        '''返回添加表情后的句子'''
        if text == '':
            return ''
        if self.use_emoji:
            emoji = self.emoji.think(text)
            return text + emoji
        elif self.use_qqface:
            id = self.qqface.think(text)
            if id != -1:
                return text + str(face(id))
        return text


    def analyze_emotion(self, text: str) -> str:
        '''返回情绪分析结果'''
        if text == '':
            return ''
        if self.use_emotion:
            return self.emotion.think(text)
        return ''


    def import_memory_dataset(self, text: str):
        '''导入记忆数据库, text 是按换行符分块的长文本'''
        if text == '':
            return
        chunks = text.split('\n\n')
        self.brain.store_memory(chunks)


    def save_memory_dataset(self, memory: str | list):
        '''保存至记忆数据库, memory 可以是文本列表, 也是可以是文本'''
        self.brain.store_memory(memory)


    def load_memory(self):
        '''读取历史记忆'''
        try:
            if not os.path.isdir('./memory'):
                os.makedirs('./memory')
            with open(f'./memory/{self.name}.json', 'r', encoding='utf-8') as f:
                dicts = json.load(f)
                self.chat_memory.messages = messages_from_dict(dicts)
                self.history.messages = messages_from_dict(dicts)
                while len(self.chat_memory.messages) > 6:
                    self.chat_memory.messages.pop(0)
                    self.chat_memory.messages.pop(0)
        except FileNotFoundError:
            pass


    def cut_memory(self):
        '''删除一轮对话'''
        for i in range(2):
            first = self.chat_memory.messages.pop(0)
            logging.debug(f'删除上下文记忆: {first}')


    def save_memory(self):
        '''保存记忆'''
        dicts = messages_to_dict(self.history.messages)
        if not os.path.isdir('./memory'):
            os.makedirs('./memory')
        with open(f'./memory/{self.name}.json', 'w',encoding='utf-8') as f:
            json.dump(dicts, f, ensure_ascii=False)


    def summarize_memory(self):
        '''总结 chat_memory 并保存到记忆数据库中'''
        prompt = ''
        for mes in self.chat_memory.messages:
            if isinstance(mes, HumanMessage):
                prompt += f'{self.username}: {mes.content}\n\n'
            elif isinstance(mes, SystemMessage):
                prompt += f'System Information: {mes.content}\n\n'
            elif isinstance(mes, AIMessage):
                prompt += f'{self.name}: {mes.content}\n\n'
        prompt_template = f"""Complete the requirements separately
Conversation:


{prompt}


      The requirements:
       - Output ONLY in Chinese.
       - Describe what the whole conversation is doing with 10 Chinese keywords in the first line. JUST send the KEYWORDS DIRECTLY:
       - What memories do you think should be important to remember from the conversation? Describe as much as possible in Chinese within 300 tokens, Each action should include the time.INCLUDEING TIME AND DATE. JUST send the SUMMARIZE DIRECTLY:"""
        print('开始总结')
        summary = self.brain.think_nonstream([SystemMessage(content=prompt_template)])
        print('结束总结')
        while len(self.chat_memory.messages) > 4:
            self.cut_memory()
        self.save_memory_dataset(summary)
        logging.info(f'总结记忆: {summary}')

#waifu-llm-brain.py#
import abc
from abc import abstractmethod

class Brain(metaclass=abc.ABCMeta):
    '''CyberWaifu's Brain, actually the interface of LLM.'''

    @abstractmethod
    def think(self, messages: list):
        pass


    @abstractmethod
    def think_nonstream(self, messages: list):
        pass


    @abstractmethod
    def store_memory(self, memory: str | list):
        pass


    @abstractmethod
    def extract_memory(self, text: str, top_n: int):
        pass

#waifu-llm-gpt.py#
from waifu.llm.Brain import Brain
from waifu.llm.VectorDB import VectorDB
from waifu.llm.SentenceTransformer import STEmbedding
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from typing import Any, List, Mapping, Optional
from langchain.schema import BaseMessage
import openai

api_base = "https://api_gpt4.ai-gaochao.com/v1"
api_base = "https://api.chatanywhere.com.cn/v1"
openai.api_base = api_base 
openai.proxy={
    "http": None,
    "https": None
}
import os
os.environ["OPENAI_API_BASE"] =api_base 


class GPT(Brain):
    def __init__(self, api_key: str,
                 name: str,
                 stream: bool=False,
                 callback=None,
                 model: str='gpt-3.5-turbo',
                 proxy: str=''):
        self.llm = ChatOpenAI(openai_api_key=api_key,
                        model_name=model,
	        openai_api_base=openai.api_base,
                        streaming=stream,
                        callbacks=[callback],
                        temperature=0.85)
        self.llm_nonstream = ChatOpenAI(openai_api_key=api_key, model_name=model, openai_api_base=openai.api_base,)
        self.embedding = OpenAIEmbeddings(openai_api_key=api_key, openai_api_base=openai.api_base,)
        # self.embedding = STEmbedding()
        self.vectordb = VectorDB(self.embedding, f'./memory/{name}.csv')
        if proxy != '':
            openai.proxy = proxy


    def think(self, messages: List[BaseMessage]):
        if not messages:
            return "Sorry, I couldn't generate a response because I didn't receive any messages."
        response = self.llm(messages)
        if response.content:
            return response.content
        else:
            # Simplify the last user message and try again
            simplified_messages = messages[:-1] + [BaseMessage(content="Simplified version of the question", role=messages[-1].role)]
            response = self.llm(simplified_messages)
            return response.content if response.content else "Sorry, I couldn't generate a response."

    def think_nonstream(self, messages: List[BaseMessage]):
        if not messages:
            return "Sorry, I couldn't generate a response because I didn't receive any messages."
        response = self.llm_nonstream(messages)
        if response.content:
            return response.content
        else:
            # Simplify the last user message and try again
            simplified_messages = messages[:-1] + [BaseMessage(content="Simplified version of the question", role=messages[-1].role)]
            response = self.llm_nonstream(simplified_messages)
            return response.content if response.content else "Sorry, I couldn't generate a response."


    def store_memory(self, text: str | list):
        '''保存记忆 embedding'''
        self.vectordb.store(text)


    def extract_memory(self, text: str, top_n: int = 10):
        '''提取 top_n 条相关记忆'''
        return self.vectordb.query(text, top_n)

#waifu-llm-sentenceTransformer.py#
from sentence_transformers import SentenceTransformer, util

from termcolor import colored

class STEmbedding():
    '''Wraper of Sentence Transformer Eembedding'''

    def __init__(self):
        try:
            self.model = SentenceTransformer('./st_model/')
        except:
            print(colored('Sentence Transformer 模型加载失败！', 'red'))


    def embed_documents(self, documents: list):
        '''返回嵌入向量'''
        return list(self.model.encode(documents).tolist())


    def embed_query(self, text: str):
        return self.model.encode(text).tolist()

#waifu-llm-VectorDB.py#
import pandas as pd
import os
import ast
from scipy import spatial

class VectorDB:
    def __init__(self, embedding, save_path):
        self.save_path = save_path
        self.embedding = embedding
        self.chunks    = []


    def store(self, text: str | list):
        '''保存 vector'''
        if isinstance(text, str):
            if text == '':
                return
            vector = self.embedding.embed_documents([text])
            df = pd.DataFrame({"text": text, "embedding": vector})
        elif isinstance(text, list):
            if len(text) == 0:
                return
            vector = self.embedding.embed_documents(text)
            df = pd.DataFrame({"text": text, "embedding": vector})
        else:
            raise TypeError('text must be str or list')
        df.to_csv(self.save_path, mode='a', header=not os.path.exists(self.save_path), index=False)


    def query(self, text: str, top_n: int, threshold: float = 0.7):
        if text == '':
            return ['']
        relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y)

        # Load embeddings data
        if not os.path.isfile(self.save_path):
            return ['']
        df = pd.read_csv(self.save_path)
        row = df.shape[0]
        top_n = min(top_n, row)
        df['embedding'] = df['embedding'].apply(ast.literal_eval)

        # Make query
        query_embedding = self.embedding.embed_query(text)
        strings_and_relatednesses = [
            (row["text"], relatedness_fn(query_embedding, row["embedding"]))
            for i, row in df.iterrows()
        ]

        # Rank
        strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)
        strings, relatednesses = zip(*strings_and_relatednesses)
        for i in range(len(relatednesses)):
            if relatednesses[i] < threshold:
                break
        return strings[:min(i+1, top_n)], relatednesses[:min(i+1, top_n)]